seed: 42
name: "blt-entropy-fine_web_edu-4096ctx"
dump_dir: "./checkpoints"

training:
  epochs: 1  # Number of epochs to train for
  val_interval: 1000  # How often to run a validation-generation step

model:
  # Transformer configuration parameters
  dim: 768
  n_layers: 14
  n_heads: 12
  head_dim: null         # Leave null to compute as dim // n_heads
  n_kv_heads: null       # Optional (default is same as n_heads)
  vocab_size: 260        # 256 bytes + reserved tokens (BOS/EOS and extras)
  ffn_dim_multiplier: 1.0
  multiple_of: 256
  norm_eps: 1e-5
  rope_theta: 10000.0
  rope_use_fp32_in_outer_product: false
  init_base_std: null    # Optional; if null, uses default initialization std
  init_std_factor: "disabled"  # Options: "disabled", "global_depth", "current_depth", "dim_ratio"
  max_seqlen: 4096       # Maximum sequence length
  attn_impl: "xformers"   # Options include "xformers", "sdpa", etc.
  attn_bias_type: "local_block_causal"
  sliding_window: 512     # Set to null if not using sliding-window attention
  weight_tying: false

data:
  train_dir: "~/Documents/fine_web_edu_latest_shuff/train"  # Directory containing training files
  val_file: "~/Documents/fine_web_edu_latest_shuff/val.jsonl"  # Validation file path
  seq_len: 4096
  batch_size: 6
  workers: 16
  max_val_samples: 4  # Maximum number of validation samples to load

optim:
  lr: 4e-4
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.1
  clip: 10.0
  warmup: 500
  lr_min_ratio: 0.1
  enable_dynamo: false

logging:
  wandb_project: "open-blt"
  log_freq: 1
  wandb_run_id: ""  # Set this to a specific run ID to resume, or leave blank for a new run

checkpoint:
  dump:
    every: 1000
    keep: 3
  resume_from: ""  # Provide path to checkpoint file to resume training, or leave blank to start fresh
