{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af4da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25d8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotEntropiesConfig(BaseModel):\n",
    "    data_path: str | None\n",
    "    chart_path: str\n",
    "    score_override_path: str | None = None\n",
    "    threshold_override: float | None = None\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "\n",
    "class PlotEntropiesData(BaseModel):\n",
    "    text: str\n",
    "    threshold: float = 1.335442066192627\n",
    "    dataframe_json: str | None\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3188a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for 'Hello, world!': [257, 72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33, 258]\n"
     ]
    }
   ],
   "source": [
    "def text_to_tokens(text, bos_id=257, eos_id=258):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text into UTF-8 bytes and adds a BOS token (257)\n",
    "    at the beginning and an EOS token (258) at the end.\n",
    "    \"\"\"\n",
    "    token_list = list(text.encode(\"utf-8\"))\n",
    "    return [bos_id] + token_list + [eos_id]\n",
    "\n",
    "\n",
    "sample_text = \"Hello, world!\"\n",
    "print(\"Tokens for 'Hello, world!':\", text_to_tokens(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ceb3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(scores):\n",
    "    \"\"\"\n",
    "    Computes per-token entropy (using natural log) from logits.\n",
    "    Input:\n",
    "      scores: Tensor of shape [bs, seq_len, vocab]\n",
    "    Returns:\n",
    "      Tensor of shape [bs, seq_len] with entropy values.\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(scores, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "    p_log_p = log_probs * probs\n",
    "    return -p_log_p.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d15e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_style_patch_start_ids(entropies: torch.Tensor, threshold: float, include_next_token: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a tensor of entropies (for tokens[1:] of the input), compute patch start indices as per Meta's logic:\n",
    "      - Force patch starts at token indices 0 and 1.\n",
    "      - For subsequent tokens, flag a patch start if the entropy > threshold.\n",
    "      \n",
    "    Parameters:\n",
    "      entropies: Tensor of shape [seq_len-1] (corresponding to tokens[1:]).\n",
    "      threshold: Threshold for flagging a patch start.\n",
    "      include_next_token: Whether to include the final token.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor of patch start indices.\n",
    "    \"\"\"\n",
    "    # Force patch starts at positions 0 and 1.\n",
    "    first_ids = torch.tensor([0, 1], dtype=torch.long, device=entropies.device)\n",
    "    # entropies corresponds to tokens[1:].\n",
    "    patch_start_mask = (entropies > threshold)\n",
    "    if not include_next_token and patch_start_mask.numel() > 1:\n",
    "        patch_start_mask = patch_start_mask[:-1]  # Optionally drop the final token.\n",
    "    patch_indices = torch.nonzero(patch_start_mask, as_tuple=False).squeeze(1)\n",
    "    patch_indices = patch_indices + 1  # Shift indices to align with the original tokens.\n",
    "    patch_start_ids = torch.cat((first_ids, patch_indices))\n",
    "    patch_start_ids = torch.unique_consecutive(patch_start_ids)\n",
    "    return patch_start_ids\n",
    "\n",
    "def patch_lengths_from_start_ids(patch_start_ids: torch.Tensor, seq_len: int) -> list:\n",
    "    \"\"\"\n",
    "    Given patch start indices and the total sequence length, compute patch lengths.\n",
    "    \n",
    "    For example, if patch_start_ids = [0, 5, 10] and seq_len = 15,\n",
    "    then patch lengths are [5, 5, 5].\n",
    "    \"\"\"\n",
    "    patch_ids = patch_start_ids.tolist()\n",
    "    patch_ids.append(seq_len)\n",
    "    lengths = []\n",
    "    for i in range(len(patch_ids)-1):\n",
    "        lengths.append(patch_ids[i+1] - patch_ids[i])\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df754075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy_and_patches(text: str, threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the entropy model, tokenizes the input text (adding BOS/EOS tokens),\n",
    "    performs a forward pass to compute next-token logits, computes per-token\n",
    "    entropy (in nats), and determines patch-start flags using Meta's patching logic.\n",
    "    \n",
    "    Alignment:\n",
    "      - The point at row i in the DataFrame represents the entropy used to predict token i+1.\n",
    "      - For i in [0, ..., N-2]: entropy is computed; for the final token (EOS, row N-1) we predict 0.\n",
    "      - patch_start[0] = 1 (BOS always starts a patch).\n",
    "      - If entropy[i] > threshold, then patch_start[i+1] = 1.\n",
    "    \n",
    "    Also, when converting tokens to strings:\n",
    "      - BOS token (257) is shown as \"<\"\n",
    "      - EOS token (258) is shown as \">\"\n",
    "    \n",
    "    Returns a DataFrame with columns:\n",
    "      'position'  : token position (0..N-1, including BOS/EOS),\n",
    "      'tokens'    : token (converted to a character, with BOS/EOS replaced),\n",
    "      'entropies' : the entropy for predicting the next token (0 for EOS),\n",
    "      'start'     : binary flag (1 indicates a patch start).\n",
    "    \"\"\"\n",
    "    # --- Model loading ---\n",
    "    model_config = OmegaConf.load(\"blt/configs/entropy.yaml\")\n",
    "    from blt.model.entropy import EntropyModel\n",
    "    model = EntropyModel(model_config)\n",
    "    model.eval()\n",
    "    model.to(\"cuda\")\n",
    "    \n",
    "    checkpoint_path = \"checkpoints/blt-entropy-pile/checkpoint_latest.pt\"\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=\"cuda\")\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
    "    \n",
    "    # --- Tokenization ---\n",
    "    tokens = text_to_tokens(text, bos_id=257, eos_id=258)\n",
    "    N = len(tokens)  # total tokens (including BOS/EOS)\n",
    "    input_ids = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(\"cuda\")  # Shape: [1, N]\n",
    "    \n",
    "    # --- Forward Pass ---\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids)  # Shape: [1, N, vocab_size]\n",
    "    \n",
    "    # --- Next-token Predictions & Entropy ---\n",
    "    # We skip the final token (no next-token prediction) so logits_next has shape [1, N-1, vocab_size].\n",
    "    logits_next = logits[:, :-1, :]\n",
    "    ent = entropy(logits_next).squeeze(0)  # Shape: [N-1]\n",
    "    # ent[i] is the entropy used to predict token i+1.\n",
    "    \n",
    "    # --- Build DataFrame (N rows) ---\n",
    "    token_strs = []\n",
    "    for t in tokens:\n",
    "        if t == 257:\n",
    "            token_strs.append(\"<\")\n",
    "        elif t == 258:\n",
    "            token_strs.append(\">\")\n",
    "        else:\n",
    "            try:\n",
    "                token_strs.append(chr(t))\n",
    "            except Exception:\n",
    "                token_strs.append(str(t))\n",
    "    \n",
    "    entropies_list = []\n",
    "    for i in range(N):\n",
    "        if i < N - 1:\n",
    "            e_val = float(ent[i])\n",
    "        else:\n",
    "            e_val = 0  # For the final token (EOS) predict 0.\n",
    "        entropies_list.append(e_val)\n",
    "    \n",
    "    # --- Patch Start Determination ---\n",
    "    # patch_start[0] = 1 (BOS always starts a patch)\n",
    "    # If ent[i] > threshold, then patch_start[i+1] = 1.\n",
    "    patch_start = [0] * N\n",
    "    patch_start[0] = 1\n",
    "    for i in range(N - 1):\n",
    "        if ent[i] > threshold:\n",
    "            patch_start[i + 1] = 1\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"position\": list(range(N)),\n",
    "        \"tokens\": token_strs,\n",
    "        \"entropies\": entropies_list,\n",
    "        \"start\": patch_start,\n",
    "    })\n",
    "    \n",
    "    patch_ids = meta_style_patch_start_ids(ent, threshold, include_next_token=False)\n",
    "    patch_lengths = patch_lengths_from_start_ids(patch_ids, seq_len=N)\n",
    "    print(\"Computed patch lengths:\", patch_lengths)\n",
    "    print(\"Patch start indices:\", patch_ids.tolist())\n",
    "    print(\"Mean patch length:\", sum(patch_lengths) / len(patch_lengths))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90478680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed patch lengths: [1, 3, 1, 2, 3, 2, 1, 2, 1, 1, 3, 6, 2, 3, 3, 9, 2, 8, 5, 3, 7, 5, 8]\n",
      "Patch start indices: [0, 1, 4, 5, 7, 10, 12, 13, 15, 16, 17, 20, 26, 28, 31, 34, 43, 45, 53, 58, 61, 68, 73]\n",
      "Mean patch length: 3.5217391304347827\n",
      "Computed DataFrame\n",
      "Chart saved to our_chart.png\n",
      "threshold: 2.5\n",
      "Mean entropy (our model): 1.653603274697139\n",
      "Total patches (our model): 23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-7e749723581f4f24b2cb50e1dcffdf9f.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-7e749723581f4f24b2cb50e1dcffdf9f.vega-embed details,\n",
       "  #altair-viz-7e749723581f4f24b2cb50e1dcffdf9f.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-7e749723581f4f24b2cb50e1dcffdf9f\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7e749723581f4f24b2cb50e1dcffdf9f\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7e749723581f4f24b2cb50e1dcffdf9f\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 15, \"titleFontSize\": 15}}, \"layer\": [{\"data\": {\"name\": \"data-2f69b9754c33d18c5082236bc8426f00\"}, \"mark\": {\"type\": \"rule\", \"color\": \"#474747\", \"strokeDash\": [4, 2]}, \"encoding\": {\"x\": {\"axis\": {\"grid\": false, \"labelAngle\": 0, \"labelExpr\": \"split(datum.label, '|')[1]\", \"labelOverlap\": false}, \"field\": \"position_with_token\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-190c5a8f5459ba8fdce03f3b2ea07e2b\"}, \"mark\": {\"type\": \"rule\", \"color\": \"red\", \"strokeDash\": [4, 4]}, \"encoding\": {\"y\": {\"datum\": 2.5}}}, {\"data\": {\"name\": \"data-190c5a8f5459ba8fdce03f3b2ea07e2b\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"x\": {\"axis\": {\"grid\": false, \"labelAngle\": 0, \"labelExpr\": \"split(datum.label, '|')[1]\", \"labelOverlap\": false}, \"field\": \"position_with_token\", \"title\": null, \"type\": \"ordinal\"}, \"y\": {\"field\": \"entropies\", \"title\": \"Entropy of Next Token\", \"type\": \"quantitative\"}}}], \"height\": 150, \"width\": 1200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-2f69b9754c33d18c5082236bc8426f00\": [{\"position\": 0, \"tokens\": \"<\", \"entropies\": 3.5107057094573975, \"start\": 1, \"position_with_token\": \"000|<\"}, {\"position\": 1, \"tokens\": \"D\", \"entropies\": 2.0769095420837402, \"start\": 1, \"position_with_token\": \"001|D\"}, {\"position\": 4, \"tokens\": \"n\", \"entropies\": 2.6021251678466797, \"start\": 1, \"position_with_token\": \"004|n\"}, {\"position\": 5, \"tokens\": \"e\", \"entropies\": 2.2380995750427246, \"start\": 1, \"position_with_token\": \"005|e\"}, {\"position\": 7, \"tokens\": \"y\", \"entropies\": 2.092273473739624, \"start\": 1, \"position_with_token\": \"007|y\"}, {\"position\": 10, \"tokens\": \"T\", \"entropies\": 2.234435796737671, \"start\": 1, \"position_with_token\": \"010|T\"}, {\"position\": 12, \"tokens\": \"r\", \"entropies\": 2.662418842315674, \"start\": 1, \"position_with_token\": \"012|r\"}, {\"position\": 13, \"tokens\": \"g\", \"entropies\": 0.4984862506389618, \"start\": 1, \"position_with_token\": \"013|g\"}, {\"position\": 15, \"tokens\": \"r\", \"entropies\": 2.8272881507873535, \"start\": 1, \"position_with_token\": \"015|r\"}, {\"position\": 16, \"tokens\": \"y\", \"entropies\": 2.784846544265747, \"start\": 1, \"position_with_token\": \"016|y\"}, {\"position\": 17, \"tokens\": \"e\", \"entropies\": 2.3973848819732666, \"start\": 1, \"position_with_token\": \"017|e\"}, {\"position\": 20, \"tokens\": \"i\", \"entropies\": 0.816152811050415, \"start\": 1, \"position_with_token\": \"020|i\"}, {\"position\": 26, \"tokens\": \"G\", \"entropies\": 2.134199619293213, \"start\": 1, \"position_with_token\": \"026|G\"}, {\"position\": 28, \"tokens\": \"m\", \"entropies\": 1.1923376321792603, \"start\": 1, \"position_with_token\": \"028|m\"}, {\"position\": 31, \"tokens\": \"o\", \"entropies\": 0.04944123700261116, \"start\": 1, \"position_with_token\": \"031|o\"}, {\"position\": 34, \"tokens\": \"T\", \"entropies\": 0.8491695523262024, \"start\": 1, \"position_with_token\": \"034|T\"}, {\"position\": 43, \"tokens\": \"a\", \"entropies\": 1.2672955989837646, \"start\": 1, \"position_with_token\": \"043|a\"}, {\"position\": 45, \"tokens\": \"f\", \"entropies\": 1.7656669616699219, \"start\": 1, \"position_with_token\": \"045|f\"}, {\"position\": 53, \"tokens\": \"e\", \"entropies\": 2.096010446548462, \"start\": 1, \"position_with_token\": \"053|e\"}, {\"position\": 58, \"tokens\": \"b\", \"entropies\": 1.9803392887115479, \"start\": 1, \"position_with_token\": \"058|b\"}, {\"position\": 61, \"tokens\": \"G\", \"entropies\": 2.0401203632354736, \"start\": 1, \"position_with_token\": \"061|G\"}, {\"position\": 68, \"tokens\": \"R\", \"entropies\": 1.8297436237335205, \"start\": 1, \"position_with_token\": \"068|R\"}, {\"position\": 73, \"tokens\": \"M\", \"entropies\": 1.7938824892044067, \"start\": 1, \"position_with_token\": \"073|M\"}], \"data-190c5a8f5459ba8fdce03f3b2ea07e2b\": [{\"position\": 0, \"tokens\": \"<\", \"entropies\": 3.5107057094573975, \"start\": 1, \"position_with_token\": \"000|<\"}, {\"position\": 1, \"tokens\": \"D\", \"entropies\": 2.0769095420837402, \"start\": 1, \"position_with_token\": \"001|D\"}, {\"position\": 2, \"tokens\": \"a\", \"entropies\": 2.3970561027526855, \"start\": 0, \"position_with_token\": \"002|a\"}, {\"position\": 3, \"tokens\": \"e\", \"entropies\": 3.0192317962646484, \"start\": 0, \"position_with_token\": \"003|e\"}, {\"position\": 4, \"tokens\": \"n\", \"entropies\": 2.6021251678466797, \"start\": 1, \"position_with_token\": \"004|n\"}, {\"position\": 5, \"tokens\": \"e\", \"entropies\": 2.2380995750427246, \"start\": 1, \"position_with_token\": \"005|e\"}, {\"position\": 6, \"tokens\": \"r\", \"entropies\": 2.5290818214416504, \"start\": 0, \"position_with_token\": \"006|r\"}, {\"position\": 7, \"tokens\": \"y\", \"entropies\": 2.092273473739624, \"start\": 1, \"position_with_token\": \"007|y\"}, {\"position\": 8, \"tokens\": \"s\", \"entropies\": 1.7212448120117188, \"start\": 0, \"position_with_token\": \"008|s\"}, {\"position\": 9, \"tokens\": \" \", \"entropies\": 3.6943323612213135, \"start\": 0, \"position_with_token\": \"009| \"}, {\"position\": 10, \"tokens\": \"T\", \"entropies\": 2.234435796737671, \"start\": 1, \"position_with_token\": \"010|T\"}, {\"position\": 11, \"tokens\": \"a\", \"entropies\": 2.9436440467834473, \"start\": 0, \"position_with_token\": \"011|a\"}, {\"position\": 12, \"tokens\": \"r\", \"entropies\": 2.662418842315674, \"start\": 1, \"position_with_token\": \"012|r\"}, {\"position\": 13, \"tokens\": \"g\", \"entropies\": 0.4984862506389618, \"start\": 1, \"position_with_token\": \"013|g\"}, {\"position\": 14, \"tokens\": \"a\", \"entropies\": 2.7227423191070557, \"start\": 0, \"position_with_token\": \"014|a\"}, {\"position\": 15, \"tokens\": \"r\", \"entropies\": 2.8272881507873535, \"start\": 1, \"position_with_token\": \"015|r\"}, {\"position\": 16, \"tokens\": \"y\", \"entropies\": 2.784846544265747, \"start\": 1, \"position_with_token\": \"016|y\"}, {\"position\": 17, \"tokens\": \"e\", \"entropies\": 2.3973848819732666, \"start\": 1, \"position_with_token\": \"017|e\"}, {\"position\": 18, \"tokens\": \"n\", \"entropies\": 2.401061773300171, \"start\": 0, \"position_with_token\": \"018|n\"}, {\"position\": 19, \"tokens\": \" \", \"entropies\": 3.7707808017730713, \"start\": 0, \"position_with_token\": \"019| \"}, {\"position\": 20, \"tokens\": \"i\", \"entropies\": 0.816152811050415, \"start\": 1, \"position_with_token\": \"020|i\"}, {\"position\": 21, \"tokens\": \"s\", \"entropies\": 0.09851308166980743, \"start\": 0, \"position_with_token\": \"021|s\"}, {\"position\": 22, \"tokens\": \" \", \"entropies\": 2.4805405139923096, \"start\": 0, \"position_with_token\": \"022| \"}, {\"position\": 23, \"tokens\": \"i\", \"entropies\": 0.5705469846725464, \"start\": 0, \"position_with_token\": \"023|i\"}, {\"position\": 24, \"tokens\": \"n\", \"entropies\": 1.6186861991882324, \"start\": 0, \"position_with_token\": \"024|n\"}, {\"position\": 25, \"tokens\": \" \", \"entropies\": 2.931950092315674, \"start\": 0, \"position_with_token\": \"025| \"}, {\"position\": 26, \"tokens\": \"G\", \"entropies\": 2.134199619293213, \"start\": 1, \"position_with_token\": \"026|G\"}, {\"position\": 27, \"tokens\": \"a\", \"entropies\": 2.5662753582000732, \"start\": 0, \"position_with_token\": \"027|a\"}, {\"position\": 28, \"tokens\": \"m\", \"entropies\": 1.1923376321792603, \"start\": 1, \"position_with_token\": \"028|m\"}, {\"position\": 29, \"tokens\": \"e\", \"entropies\": 1.7752230167388916, \"start\": 0, \"position_with_token\": \"029|e\"}, {\"position\": 30, \"tokens\": \" \", \"entropies\": 3.1512961387634277, \"start\": 0, \"position_with_token\": \"030| \"}, {\"position\": 31, \"tokens\": \"o\", \"entropies\": 0.04944123700261116, \"start\": 1, \"position_with_token\": \"031|o\"}, {\"position\": 32, \"tokens\": \"f\", \"entropies\": 0.025913041085004807, \"start\": 0, \"position_with_token\": \"032|f\"}, {\"position\": 33, \"tokens\": \" \", \"entropies\": 2.508931875228882, \"start\": 0, \"position_with_token\": \"033| \"}, {\"position\": 34, \"tokens\": \"T\", \"entropies\": 0.8491695523262024, \"start\": 1, \"position_with_token\": \"034|T\"}, {\"position\": 35, \"tokens\": \"h\", \"entropies\": 0.32687729597091675, \"start\": 0, \"position_with_token\": \"035|h\"}, {\"position\": 36, \"tokens\": \"r\", \"entropies\": 0.7737484574317932, \"start\": 0, \"position_with_token\": \"036|r\"}, {\"position\": 37, \"tokens\": \"o\", \"entropies\": 0.41398102045059204, \"start\": 0, \"position_with_token\": \"037|o\"}, {\"position\": 38, \"tokens\": \"n\", \"entropies\": 0.03202815353870392, \"start\": 0, \"position_with_token\": \"038|n\"}, {\"position\": 39, \"tokens\": \"e\", \"entropies\": 0.6802260279655457, \"start\": 0, \"position_with_token\": \"039|e\"}, {\"position\": 40, \"tokens\": \"s\", \"entropies\": 1.3742433786392212, \"start\": 0, \"position_with_token\": \"040|s\"}, {\"position\": 41, \"tokens\": \",\", \"entropies\": 0.1409749835729599, \"start\": 0, \"position_with_token\": \"041|,\"}, {\"position\": 42, \"tokens\": \" \", \"entropies\": 3.370836019515991, \"start\": 0, \"position_with_token\": \"042| \"}, {\"position\": 43, \"tokens\": \"a\", \"entropies\": 1.2672955989837646, \"start\": 1, \"position_with_token\": \"043|a\"}, {\"position\": 44, \"tokens\": \" \", \"entropies\": 3.2301125526428223, \"start\": 0, \"position_with_token\": \"044| \"}, {\"position\": 45, \"tokens\": \"f\", \"entropies\": 1.7656669616699219, \"start\": 1, \"position_with_token\": \"045|f\"}, {\"position\": 46, \"tokens\": \"a\", \"entropies\": 2.125420331954956, \"start\": 0, \"position_with_token\": \"046|a\"}, {\"position\": 47, \"tokens\": \"n\", \"entropies\": 1.1103873252868652, \"start\": 0, \"position_with_token\": \"047|n\"}, {\"position\": 48, \"tokens\": \"t\", \"entropies\": 0.019940713420510292, \"start\": 0, \"position_with_token\": \"048|t\"}, {\"position\": 49, \"tokens\": \"a\", \"entropies\": 0.03044786863029003, \"start\": 0, \"position_with_token\": \"049|a\"}, {\"position\": 50, \"tokens\": \"s\", \"entropies\": 1.1462219953536987, \"start\": 0, \"position_with_token\": \"050|s\"}, {\"position\": 51, \"tokens\": \"y\", \"entropies\": 0.27408701181411743, \"start\": 0, \"position_with_token\": \"051|y\"}, {\"position\": 52, \"tokens\": \" \", \"entropies\": 3.0526063442230225, \"start\": 0, \"position_with_token\": \"052| \"}, {\"position\": 53, \"tokens\": \"e\", \"entropies\": 2.096010446548462, \"start\": 1, \"position_with_token\": \"053|e\"}, {\"position\": 54, \"tokens\": \"p\", \"entropies\": 0.24165232479572296, \"start\": 0, \"position_with_token\": \"054|p\"}, {\"position\": 55, \"tokens\": \"i\", \"entropies\": 0.8399909138679504, \"start\": 0, \"position_with_token\": \"055|i\"}, {\"position\": 56, \"tokens\": \"c\", \"entropies\": 0.5739892721176147, \"start\": 0, \"position_with_token\": \"056|c\"}, {\"position\": 57, \"tokens\": \" \", \"entropies\": 3.075429916381836, \"start\": 0, \"position_with_token\": \"057| \"}, {\"position\": 58, \"tokens\": \"b\", \"entropies\": 1.9803392887115479, \"start\": 1, \"position_with_token\": \"058|b\"}, {\"position\": 59, \"tokens\": \"y\", \"entropies\": 0.13878144323825836, \"start\": 0, \"position_with_token\": \"059|y\"}, {\"position\": 60, \"tokens\": \" \", \"entropies\": 3.5570616722106934, \"start\": 0, \"position_with_token\": \"060| \"}, {\"position\": 61, \"tokens\": \"G\", \"entropies\": 2.0401203632354736, \"start\": 1, \"position_with_token\": \"061|G\"}, {\"position\": 62, \"tokens\": \"e\", \"entropies\": 1.8886935710906982, \"start\": 0, \"position_with_token\": \"062|e\"}, {\"position\": 63, \"tokens\": \"o\", \"entropies\": 0.614566445350647, \"start\": 0, \"position_with_token\": \"063|o\"}, {\"position\": 64, \"tokens\": \"r\", \"entropies\": 0.02335818111896515, \"start\": 0, \"position_with_token\": \"064|r\"}, {\"position\": 65, \"tokens\": \"g\", \"entropies\": 0.7958493232727051, \"start\": 0, \"position_with_token\": \"065|g\"}, {\"position\": 66, \"tokens\": \"e\", \"entropies\": 0.5956071615219116, \"start\": 0, \"position_with_token\": \"066|e\"}, {\"position\": 67, \"tokens\": \" \", \"entropies\": 3.004364490509033, \"start\": 0, \"position_with_token\": \"067| \"}, {\"position\": 68, \"tokens\": \"R\", \"entropies\": 1.8297436237335205, \"start\": 1, \"position_with_token\": \"068|R\"}, {\"position\": 69, \"tokens\": \".\", \"entropies\": 0.48682087659835815, \"start\": 0, \"position_with_token\": \"069|.\"}, {\"position\": 70, \"tokens\": \"R\", \"entropies\": 1.0099756717681885, \"start\": 0, \"position_with_token\": \"070|R\"}, {\"position\": 71, \"tokens\": \".\", \"entropies\": 0.9865317344665527, \"start\": 0, \"position_with_token\": \"071|.\"}, {\"position\": 72, \"tokens\": \" \", \"entropies\": 3.5253872871398926, \"start\": 0, \"position_with_token\": \"072| \"}, {\"position\": 73, \"tokens\": \"M\", \"entropies\": 1.7938824892044067, \"start\": 1, \"position_with_token\": \"073|M\"}, {\"position\": 74, \"tokens\": \"a\", \"entropies\": 2.236363172531128, \"start\": 0, \"position_with_token\": \"074|a\"}, {\"position\": 75, \"tokens\": \"r\", \"entropies\": 2.240882158279419, \"start\": 0, \"position_with_token\": \"075|r\"}, {\"position\": 76, \"tokens\": \"t\", \"entropies\": 0.5480268597602844, \"start\": 0, \"position_with_token\": \"076|t\"}, {\"position\": 77, \"tokens\": \"i\", \"entropies\": 0.10690046101808548, \"start\": 0, \"position_with_token\": \"077|i\"}, {\"position\": 78, \"tokens\": \"n\", \"entropies\": 1.8413935899734497, \"start\": 0, \"position_with_token\": \"078|n\"}, {\"position\": 79, \"tokens\": \".\", \"entropies\": 0.841713547706604, \"start\": 0, \"position_with_token\": \"079|.\"}, {\"position\": 80, \"tokens\": \">\", \"entropies\": 0.0, \"start\": 0, \"position_with_token\": \"080|>\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"Daenerys Targaryen is in Game of Thrones, a fantasy epic by George R.R. Martin.\"\n",
    "threshold = 2.5  # Meta's documented threshold\n",
    "df = compute_entropy_and_patches(sample_sentence, threshold)\n",
    "print(\"Computed DataFrame\")\n",
    "\n",
    "# Create x-axis tick labels combining position and token.\n",
    "x_ticks = []\n",
    "for row in df.itertuples():\n",
    "    pos = row.position\n",
    "    token = row.tokens\n",
    "    x_ticks.append(f\"{str(pos).zfill(3)}|{token}\")\n",
    "df[\"position_with_token\"] = x_ticks\n",
    "\n",
    "# Configure the Altair x-axis to split the label at \"|\" and show only the token.\n",
    "x_axis = alt.Axis(\n",
    "    labelExpr=\"split(datum.label, '|')[1]\",\n",
    "    grid=False,\n",
    "    labelOverlap=False,\n",
    "    labelAngle=0,\n",
    ")\n",
    "\n",
    "width = 1200\n",
    "height = 150\n",
    "base = alt.Chart(df).properties(width=width, height=height)\n",
    "points = base.mark_line(point=True).encode(\n",
    "    x=alt.X(\"position_with_token:O\", title=None, axis=x_axis),\n",
    "    y=alt.Y(\"entropies\", title=\"Entropy of Next Token\"),\n",
    ")\n",
    "# Draw a horizontal rule at the threshold.\n",
    "rule = base.mark_rule(color=\"red\", strokeDash=[4, 4]).encode(\n",
    "    y=alt.datum(threshold),\n",
    ")\n",
    "# Draw vertical dashed lines at patch-start positions.\n",
    "patch_rules = (\n",
    "    alt.Chart(df[df[\"start\"] > 0])\n",
    "    .mark_rule(color=\"#474747\", strokeDash=[4, 2])\n",
    "    .encode(x=alt.X(\"position_with_token:O\", axis=x_axis))\n",
    ")\n",
    "\n",
    "chart = patch_rules + rule + points\n",
    "chart = chart.configure_axis(labelFontSize=15, titleFontSize=15)\n",
    "\n",
    "output_path = Path(\"our_chart.png\")\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "chart.save(str(output_path))\n",
    "print(f\"Chart saved to {output_path}\")\n",
    "\n",
    "print(f\"threshold: {threshold}\")\n",
    "print(f\"Mean entropy (our model): {df['entropies'].mean()}\")\n",
    "print(f\"Total patches (our model): {sum(df['start'])}\")\n",
    "chart  # Display the chart inline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ff214",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "#### Meta's Documented Patch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d484d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame:\n",
      "Chart saved to meta_chart.png\n",
      "threshold: 1.335442066192627\n",
      "Mean entropy (metas model): 0.8172790294580247\n",
      "Total patches (metas model): 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51345/2580662555.py:10: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_json(data[\"dataframe_json\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-63e12dd5176f4e259d4aaffbd2c0bb8f.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-63e12dd5176f4e259d4aaffbd2c0bb8f.vega-embed details,\n",
       "  #altair-viz-63e12dd5176f4e259d4aaffbd2c0bb8f.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-63e12dd5176f4e259d4aaffbd2c0bb8f\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-63e12dd5176f4e259d4aaffbd2c0bb8f\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-63e12dd5176f4e259d4aaffbd2c0bb8f\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 15, \"titleFontSize\": 15}}, \"layer\": [{\"data\": {\"name\": \"data-2427c7ddf3873183a6f964529fb41310\"}, \"mark\": {\"type\": \"rule\", \"color\": \"#474747\", \"strokeDash\": [4, 2]}, \"encoding\": {\"x\": {\"axis\": {\"grid\": false, \"labelAngle\": 0, \"labelExpr\": \"split(datum.label, '|')[1]\", \"labelOverlap\": false}, \"field\": \"position_with_token\", \"type\": \"ordinal\"}}}, {\"data\": {\"name\": \"data-d7ea15b330ad476498c33dcd3a94e191\"}, \"mark\": {\"type\": \"rule\", \"color\": \"red\", \"strokeDash\": [4, 4]}, \"encoding\": {\"y\": {\"datum\": 1.335442066192627}}}, {\"data\": {\"name\": \"data-d7ea15b330ad476498c33dcd3a94e191\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"x\": {\"axis\": {\"grid\": false, \"labelAngle\": 0, \"labelExpr\": \"split(datum.label, '|')[1]\", \"labelOverlap\": false}, \"field\": \"position_with_token\", \"title\": null, \"type\": \"ordinal\"}, \"y\": {\"field\": \"entropies\", \"title\": \"Entropy of Next Byte\", \"type\": \"quantitative\"}}}], \"height\": 150, \"width\": 1200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-2427c7ddf3873183a6f964529fb41310\": [{\"position\": 0, \"tokens\": \"<\", \"token_ids\": 1, \"entropies\": 3.3949158192, \"patch\": 0, \"start\": 1, \"position_with_token\": \"000|<\"}, {\"position\": 1, \"tokens\": \"D\", \"token_ids\": 72, \"entropies\": 2.1656451225, \"patch\": 1, \"start\": 1, \"position_with_token\": \"001|D\"}, {\"position\": 2, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 2.3216569424, \"patch\": 2, \"start\": 1, \"position_with_token\": \"002|a\"}, {\"position\": 3, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 2.8214058876, \"patch\": 3, \"start\": 1, \"position_with_token\": \"003|e\"}, {\"position\": 4, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 1.5249242782999999, \"patch\": 4, \"start\": 1, \"position_with_token\": \"004|n\"}, {\"position\": 5, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.040162414300000004, \"patch\": 5, \"start\": 1, \"position_with_token\": \"005|e\"}, {\"position\": 20, \"tokens\": \"i\", \"token_ids\": 109, \"entropies\": 0.4502205253, \"patch\": 6, \"start\": 1, \"position_with_token\": \"020|i\"}, {\"position\": 23, \"tokens\": \"i\", \"token_ids\": 109, \"entropies\": 0.3789347112, \"patch\": 7, \"start\": 1, \"position_with_token\": \"023|i\"}, {\"position\": 26, \"tokens\": \"G\", \"token_ids\": 75, \"entropies\": 1.8933598995, \"patch\": 8, \"start\": 1, \"position_with_token\": \"026|G\"}, {\"position\": 27, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 1.3859074116, \"patch\": 9, \"start\": 1, \"position_with_token\": \"027|a\"}, {\"position\": 28, \"tokens\": \"m\", \"token_ids\": 113, \"entropies\": 0.3827198744, \"patch\": 10, \"start\": 1, \"position_with_token\": \"028|m\"}, {\"position\": 31, \"tokens\": \"o\", \"token_ids\": 115, \"entropies\": 0.013672782100000001, \"patch\": 11, \"start\": 1, \"position_with_token\": \"031|o\"}, {\"position\": 43, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 1.3883389235, \"patch\": 12, \"start\": 1, \"position_with_token\": \"043|a\"}, {\"position\": 44, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 3.0503094196, \"patch\": 13, \"start\": 1, \"position_with_token\": \"044|_\"}, {\"position\": 45, \"tokens\": \"f\", \"token_ids\": 106, \"entropies\": 1.695879817, \"patch\": 14, \"start\": 1, \"position_with_token\": \"045|f\"}, {\"position\": 46, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 1.8551058769000002, \"patch\": 15, \"start\": 1, \"position_with_token\": \"046|a\"}, {\"position\": 47, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 1.4570231438, \"patch\": 16, \"start\": 1, \"position_with_token\": \"047|n\"}, {\"position\": 48, \"tokens\": \"t\", \"token_ids\": 120, \"entropies\": 0.0047810897, \"patch\": 17, \"start\": 1, \"position_with_token\": \"048|t\"}, {\"position\": 53, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 1.143143177, \"patch\": 18, \"start\": 1, \"position_with_token\": \"053|e\"}, {\"position\": 58, \"tokens\": \"b\", \"token_ids\": 102, \"entropies\": 1.3370712996, \"patch\": 19, \"start\": 1, \"position_with_token\": \"058|b\"}, {\"position\": 59, \"tokens\": \"y\", \"token_ids\": 125, \"entropies\": 0.022717354800000002, \"patch\": 20, \"start\": 1, \"position_with_token\": \"059|y\"}, {\"position\": 61, \"tokens\": \"G\", \"token_ids\": 75, \"entropies\": 1.8576486111000001, \"patch\": 21, \"start\": 1, \"position_with_token\": \"061|G\"}, {\"position\": 62, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.8189754486, \"patch\": 22, \"start\": 1, \"position_with_token\": \"062|e\"}], \"data-d7ea15b330ad476498c33dcd3a94e191\": [{\"position\": 0, \"tokens\": \"<\", \"token_ids\": 1, \"entropies\": 3.3949158192, \"patch\": 0, \"start\": 1, \"position_with_token\": \"000|<\"}, {\"position\": 1, \"tokens\": \"D\", \"token_ids\": 72, \"entropies\": 2.1656451225, \"patch\": 1, \"start\": 1, \"position_with_token\": \"001|D\"}, {\"position\": 2, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 2.3216569424, \"patch\": 2, \"start\": 1, \"position_with_token\": \"002|a\"}, {\"position\": 3, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 2.8214058876, \"patch\": 3, \"start\": 1, \"position_with_token\": \"003|e\"}, {\"position\": 4, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 1.5249242782999999, \"patch\": 4, \"start\": 1, \"position_with_token\": \"004|n\"}, {\"position\": 5, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.040162414300000004, \"patch\": 5, \"start\": 1, \"position_with_token\": \"005|e\"}, {\"position\": 6, \"tokens\": \"r\", \"token_ids\": 118, \"entropies\": 0.0981037766, \"patch\": 5, \"start\": 0, \"position_with_token\": \"006|r\"}, {\"position\": 7, \"tokens\": \"y\", \"token_ids\": 125, \"entropies\": 0.0544578359, \"patch\": 5, \"start\": 0, \"position_with_token\": \"007|y\"}, {\"position\": 8, \"tokens\": \"s\", \"token_ids\": 119, \"entropies\": 0.3430138826, \"patch\": 5, \"start\": 0, \"position_with_token\": \"008|s\"}, {\"position\": 9, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 1.0546212196, \"patch\": 5, \"start\": 0, \"position_with_token\": \"009|_\"}, {\"position\": 10, \"tokens\": \"T\", \"token_ids\": 88, \"entropies\": 0.25252828, \"patch\": 5, \"start\": 0, \"position_with_token\": \"010|T\"}, {\"position\": 11, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 0.1494535804, \"patch\": 5, \"start\": 0, \"position_with_token\": \"011|a\"}, {\"position\": 12, \"tokens\": \"r\", \"token_ids\": 118, \"entropies\": 0.0624754503, \"patch\": 5, \"start\": 0, \"position_with_token\": \"012|r\"}, {\"position\": 13, \"tokens\": \"g\", \"token_ids\": 107, \"entropies\": 0.001355894, \"patch\": 5, \"start\": 0, \"position_with_token\": \"013|g\"}, {\"position\": 14, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 0.0050173439, \"patch\": 5, \"start\": 0, \"position_with_token\": \"014|a\"}, {\"position\": 15, \"tokens\": \"r\", \"token_ids\": 118, \"entropies\": 0.0052358187, \"patch\": 5, \"start\": 0, \"position_with_token\": \"015|r\"}, {\"position\": 16, \"tokens\": \"y\", \"token_ids\": 125, \"entropies\": 0.0011725067, \"patch\": 5, \"start\": 0, \"position_with_token\": \"016|y\"}, {\"position\": 17, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.0010307421, \"patch\": 5, \"start\": 0, \"position_with_token\": \"017|e\"}, {\"position\": 18, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 1.0241208076, \"patch\": 5, \"start\": 0, \"position_with_token\": \"018|n\"}, {\"position\": 19, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 3.6867966652, \"patch\": 5, \"start\": 0, \"position_with_token\": \"019|_\"}, {\"position\": 20, \"tokens\": \"i\", \"token_ids\": 109, \"entropies\": 0.4502205253, \"patch\": 6, \"start\": 1, \"position_with_token\": \"020|i\"}, {\"position\": 21, \"tokens\": \"s\", \"token_ids\": 119, \"entropies\": 0.0484119244, \"patch\": 6, \"start\": 0, \"position_with_token\": \"021|s\"}, {\"position\": 22, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 2.2572875023, \"patch\": 6, \"start\": 0, \"position_with_token\": \"022|_\"}, {\"position\": 23, \"tokens\": \"i\", \"token_ids\": 109, \"entropies\": 0.3789347112, \"patch\": 7, \"start\": 1, \"position_with_token\": \"023|i\"}, {\"position\": 24, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 1.0042934418, \"patch\": 7, \"start\": 0, \"position_with_token\": \"024|n\"}, {\"position\": 25, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 2.9090054035, \"patch\": 7, \"start\": 0, \"position_with_token\": \"025|_\"}, {\"position\": 26, \"tokens\": \"G\", \"token_ids\": 75, \"entropies\": 1.8933598995, \"patch\": 8, \"start\": 1, \"position_with_token\": \"026|G\"}, {\"position\": 27, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 1.3859074116, \"patch\": 9, \"start\": 1, \"position_with_token\": \"027|a\"}, {\"position\": 28, \"tokens\": \"m\", \"token_ids\": 113, \"entropies\": 0.3827198744, \"patch\": 10, \"start\": 1, \"position_with_token\": \"028|m\"}, {\"position\": 29, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.26463657620000003, \"patch\": 10, \"start\": 0, \"position_with_token\": \"029|e\"}, {\"position\": 30, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 1.7742085457, \"patch\": 10, \"start\": 0, \"position_with_token\": \"030|_\"}, {\"position\": 31, \"tokens\": \"o\", \"token_ids\": 115, \"entropies\": 0.013672782100000001, \"patch\": 11, \"start\": 1, \"position_with_token\": \"031|o\"}, {\"position\": 32, \"tokens\": \"f\", \"token_ids\": 106, \"entropies\": 0.0053820172, \"patch\": 11, \"start\": 0, \"position_with_token\": \"032|f\"}, {\"position\": 33, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 0.5485631227000001, \"patch\": 11, \"start\": 0, \"position_with_token\": \"033|_\"}, {\"position\": 34, \"tokens\": \"T\", \"token_ids\": 88, \"entropies\": 0.2064044327, \"patch\": 11, \"start\": 0, \"position_with_token\": \"034|T\"}, {\"position\": 35, \"tokens\": \"h\", \"token_ids\": 108, \"entropies\": 0.0049266233, \"patch\": 11, \"start\": 0, \"position_with_token\": \"035|h\"}, {\"position\": 36, \"tokens\": \"r\", \"token_ids\": 118, \"entropies\": 0.0005439016, \"patch\": 11, \"start\": 0, \"position_with_token\": \"036|r\"}, {\"position\": 37, \"tokens\": \"o\", \"token_ids\": 115, \"entropies\": 0.0007023578, \"patch\": 11, \"start\": 0, \"position_with_token\": \"037|o\"}, {\"position\": 38, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 0.00041703350000000004, \"patch\": 11, \"start\": 0, \"position_with_token\": \"038|n\"}, {\"position\": 39, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.0054524317000000004, \"patch\": 11, \"start\": 0, \"position_with_token\": \"039|e\"}, {\"position\": 40, \"tokens\": \"s\", \"token_ids\": 119, \"entropies\": 1.1938130856, \"patch\": 11, \"start\": 0, \"position_with_token\": \"040|s\"}, {\"position\": 41, \"tokens\": \",\", \"token_ids\": 48, \"entropies\": 0.023821519700000002, \"patch\": 11, \"start\": 0, \"position_with_token\": \"041|,\"}, {\"position\": 42, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 3.1279797554, \"patch\": 11, \"start\": 0, \"position_with_token\": \"042|_\"}, {\"position\": 43, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 1.3883389235, \"patch\": 12, \"start\": 1, \"position_with_token\": \"043|a\"}, {\"position\": 44, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 3.0503094196, \"patch\": 13, \"start\": 1, \"position_with_token\": \"044|_\"}, {\"position\": 45, \"tokens\": \"f\", \"token_ids\": 106, \"entropies\": 1.695879817, \"patch\": 14, \"start\": 1, \"position_with_token\": \"045|f\"}, {\"position\": 46, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 1.8551058769000002, \"patch\": 15, \"start\": 1, \"position_with_token\": \"046|a\"}, {\"position\": 47, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 1.4570231438, \"patch\": 16, \"start\": 1, \"position_with_token\": \"047|n\"}, {\"position\": 48, \"tokens\": \"t\", \"token_ids\": 120, \"entropies\": 0.0047810897, \"patch\": 17, \"start\": 1, \"position_with_token\": \"048|t\"}, {\"position\": 49, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 0.026396824000000003, \"patch\": 17, \"start\": 0, \"position_with_token\": \"049|a\"}, {\"position\": 50, \"tokens\": \"s\", \"token_ids\": 119, \"entropies\": 0.6633765101, \"patch\": 17, \"start\": 0, \"position_with_token\": \"050|s\"}, {\"position\": 51, \"tokens\": \"y\", \"token_ids\": 125, \"entropies\": 0.3141393065, \"patch\": 17, \"start\": 0, \"position_with_token\": \"051|y\"}, {\"position\": 52, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 2.8411159515, \"patch\": 17, \"start\": 0, \"position_with_token\": \"052|_\"}, {\"position\": 53, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 1.143143177, \"patch\": 18, \"start\": 1, \"position_with_token\": \"053|e\"}, {\"position\": 54, \"tokens\": \"p\", \"token_ids\": 116, \"entropies\": 0.0520330966, \"patch\": 18, \"start\": 0, \"position_with_token\": \"054|p\"}, {\"position\": 55, \"tokens\": \"i\", \"token_ids\": 109, \"entropies\": 0.3398066461, \"patch\": 18, \"start\": 0, \"position_with_token\": \"055|i\"}, {\"position\": 56, \"tokens\": \"c\", \"token_ids\": 103, \"entropies\": 0.41401758790000004, \"patch\": 18, \"start\": 0, \"position_with_token\": \"056|c\"}, {\"position\": 57, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 2.5563707352, \"patch\": 18, \"start\": 0, \"position_with_token\": \"057|_\"}, {\"position\": 58, \"tokens\": \"b\", \"token_ids\": 102, \"entropies\": 1.3370712996, \"patch\": 19, \"start\": 1, \"position_with_token\": \"058|b\"}, {\"position\": 59, \"tokens\": \"y\", \"token_ids\": 125, \"entropies\": 0.022717354800000002, \"patch\": 20, \"start\": 1, \"position_with_token\": \"059|y\"}, {\"position\": 60, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 3.4447185993, \"patch\": 20, \"start\": 0, \"position_with_token\": \"060|_\"}, {\"position\": 61, \"tokens\": \"G\", \"token_ids\": 75, \"entropies\": 1.8576486111000001, \"patch\": 21, \"start\": 1, \"position_with_token\": \"061|G\"}, {\"position\": 62, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.8189754486, \"patch\": 22, \"start\": 1, \"position_with_token\": \"062|e\"}, {\"position\": 63, \"tokens\": \"o\", \"token_ids\": 115, \"entropies\": 0.6776530743, \"patch\": 22, \"start\": 0, \"position_with_token\": \"063|o\"}, {\"position\": 64, \"tokens\": \"r\", \"token_ids\": 118, \"entropies\": 0.0677763447, \"patch\": 22, \"start\": 0, \"position_with_token\": \"064|r\"}, {\"position\": 65, \"tokens\": \"g\", \"token_ids\": 107, \"entropies\": 0.21271303300000002, \"patch\": 22, \"start\": 0, \"position_with_token\": \"065|g\"}, {\"position\": 66, \"tokens\": \"e\", \"token_ids\": 105, \"entropies\": 0.10034800320000001, \"patch\": 22, \"start\": 0, \"position_with_token\": \"066|e\"}, {\"position\": 67, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 0.1746164262, \"patch\": 22, \"start\": 0, \"position_with_token\": \"067|_\"}, {\"position\": 68, \"tokens\": \"R\", \"token_ids\": 86, \"entropies\": 0.4123829603, \"patch\": 22, \"start\": 0, \"position_with_token\": \"068|R\"}, {\"position\": 69, \"tokens\": \".\", \"token_ids\": 50, \"entropies\": 0.5507118702, \"patch\": 22, \"start\": 0, \"position_with_token\": \"069|.\"}, {\"position\": 70, \"tokens\": \"R\", \"token_ids\": 86, \"entropies\": 0.1047425047, \"patch\": 22, \"start\": 0, \"position_with_token\": \"070|R\"}, {\"position\": 71, \"tokens\": \".\", \"token_ids\": 50, \"entropies\": 0.0194335245, \"patch\": 22, \"start\": 0, \"position_with_token\": \"071|.\"}, {\"position\": 72, \"tokens\": \"_\", \"token_ids\": 36, \"entropies\": 0.001482119, \"patch\": 22, \"start\": 0, \"position_with_token\": \"072|_\"}, {\"position\": 73, \"tokens\": \"M\", \"token_ids\": 81, \"entropies\": 0.0009310447, \"patch\": 22, \"start\": 0, \"position_with_token\": \"073|M\"}, {\"position\": 74, \"tokens\": \"a\", \"token_ids\": 101, \"entropies\": 0.0002176317, \"patch\": 22, \"start\": 0, \"position_with_token\": \"074|a\"}, {\"position\": 75, \"tokens\": \"r\", \"token_ids\": 118, \"entropies\": 0.0076908777, \"patch\": 22, \"start\": 0, \"position_with_token\": \"075|r\"}, {\"position\": 76, \"tokens\": \"t\", \"token_ids\": 120, \"entropies\": 0.0003866984, \"patch\": 22, \"start\": 0, \"position_with_token\": \"076|t\"}, {\"position\": 77, \"tokens\": \"i\", \"token_ids\": 109, \"entropies\": 0.0008008487000000001, \"patch\": 22, \"start\": 0, \"position_with_token\": \"077|i\"}, {\"position\": 78, \"tokens\": \"n\", \"token_ids\": 114, \"entropies\": 1.2395234108, \"patch\": 22, \"start\": 0, \"position_with_token\": \"078|n\"}, {\"position\": 79, \"tokens\": \".\", \"token_ids\": 50, \"entropies\": 0.4564163089, \"patch\": 22, \"start\": 0, \"position_with_token\": \"079|.\"}, {\"position\": 80, \"tokens\": \">\", \"token_ids\": 2, \"entropies\": 4.61392e-05, \"patch\": 22, \"start\": 0, \"position_with_token\": \"080|>\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from pathlib import Path\n",
    "\n",
    "meta_json_str = r'''{\"text\":\"Daenerys Targaryen is in Game of Thrones, a fantasy epic by George R.R. Martin.\",\"threshold\":1.335442066192627,\"dataframe_json\":\"{\\\"position\\\":{\\\"0\\\":0,\\\"1\\\":1,\\\"2\\\":2,\\\"3\\\":3,\\\"4\\\":4,\\\"5\\\":5,\\\"6\\\":6,\\\"7\\\":7,\\\"8\\\":8,\\\"9\\\":9,\\\"10\\\":10,\\\"11\\\":11,\\\"12\\\":12,\\\"13\\\":13,\\\"14\\\":14,\\\"15\\\":15,\\\"16\\\":16,\\\"17\\\":17,\\\"18\\\":18,\\\"19\\\":19,\\\"20\\\":20,\\\"21\\\":21,\\\"22\\\":22,\\\"23\\\":23,\\\"24\\\":24,\\\"25\\\":25,\\\"26\\\":26,\\\"27\\\":27,\\\"28\\\":28,\\\"29\\\":29,\\\"30\\\":30,\\\"31\\\":31,\\\"32\\\":32,\\\"33\\\":33,\\\"34\\\":34,\\\"35\\\":35,\\\"36\\\":36,\\\"37\\\":37,\\\"38\\\":38,\\\"39\\\":39,\\\"40\\\":40,\\\"41\\\":41,\\\"42\\\":42,\\\"43\\\":43,\\\"44\\\":44,\\\"45\\\":45,\\\"46\\\":46,\\\"47\\\":47,\\\"48\\\":48,\\\"49\\\":49,\\\"50\\\":50,\\\"51\\\":51,\\\"52\\\":52,\\\"53\\\":53,\\\"54\\\":54,\\\"55\\\":55,\\\"56\\\":56,\\\"57\\\":57,\\\"58\\\":58,\\\"59\\\":59,\\\"60\\\":60,\\\"61\\\":61,\\\"62\\\":62,\\\"63\\\":63,\\\"64\\\":64,\\\"65\\\":65,\\\"66\\\":66,\\\"67\\\":67,\\\"68\\\":68,\\\"69\\\":69,\\\"70\\\":70,\\\"71\\\":71,\\\"72\\\":72,\\\"73\\\":73,\\\"74\\\":74,\\\"75\\\":75,\\\"76\\\":76,\\\"77\\\":77,\\\"78\\\":78,\\\"79\\\":79,\\\"80\\\":80},\\\"tokens\\\":{\\\"0\\\":\\\"<\\\",\\\"1\\\":\\\"D\\\",\\\"2\\\":\\\"a\\\",\\\"3\\\":\\\"e\\\",\\\"4\\\":\\\"n\\\",\\\"5\\\":\\\"e\\\",\\\"6\\\":\\\"r\\\",\\\"7\\\":\\\"y\\\",\\\"8\\\":\\\"s\\\",\\\"9\\\":\\\"_\\\",\\\"10\\\":\\\"T\\\",\\\"11\\\":\\\"a\\\",\\\"12\\\":\\\"r\\\",\\\"13\\\":\\\"g\\\",\\\"14\\\":\\\"a\\\",\\\"15\\\":\\\"r\\\",\\\"16\\\":\\\"y\\\",\\\"17\\\":\\\"e\\\",\\\"18\\\":\\\"n\\\",\\\"19\\\":\\\"_\\\",\\\"20\\\":\\\"i\\\",\\\"21\\\":\\\"s\\\",\\\"22\\\":\\\"_\\\",\\\"23\\\":\\\"i\\\",\\\"24\\\":\\\"n\\\",\\\"25\\\":\\\"_\\\",\\\"26\\\":\\\"G\\\",\\\"27\\\":\\\"a\\\",\\\"28\\\":\\\"m\\\",\\\"29\\\":\\\"e\\\",\\\"30\\\":\\\"_\\\",\\\"31\\\":\\\"o\\\",\\\"32\\\":\\\"f\\\",\\\"33\\\":\\\"_\\\",\\\"34\\\":\\\"T\\\",\\\"35\\\":\\\"h\\\",\\\"36\\\":\\\"r\\\",\\\"37\\\":\\\"o\\\",\\\"38\\\":\\\"n\\\",\\\"39\\\":\\\"e\\\",\\\"40\\\":\\\"s\\\",\\\"41\\\":\\\",\\\",\\\"42\\\":\\\"_\\\",\\\"43\\\":\\\"a\\\",\\\"44\\\":\\\"_\\\",\\\"45\\\":\\\"f\\\",\\\"46\\\":\\\"a\\\",\\\"47\\\":\\\"n\\\",\\\"48\\\":\\\"t\\\",\\\"49\\\":\\\"a\\\",\\\"50\\\":\\\"s\\\",\\\"51\\\":\\\"y\\\",\\\"52\\\":\\\"_\\\",\\\"53\\\":\\\"e\\\",\\\"54\\\":\\\"p\\\",\\\"55\\\":\\\"i\\\",\\\"56\\\":\\\"c\\\",\\\"57\\\":\\\"_\\\",\\\"58\\\":\\\"b\\\",\\\"59\\\":\\\"y\\\",\\\"60\\\":\\\"_\\\",\\\"61\\\":\\\"G\\\",\\\"62\\\":\\\"e\\\",\\\"63\\\":\\\"o\\\",\\\"64\\\":\\\"r\\\",\\\"65\\\":\\\"g\\\",\\\"66\\\":\\\"e\\\",\\\"67\\\":\\\"_\\\",\\\"68\\\":\\\"R\\\",\\\"69\\\":\\\".\\\",\\\"70\\\":\\\"R\\\",\\\"71\\\":\\\".\\\",\\\"72\\\":\\\"_\\\",\\\"73\\\":\\\"M\\\",\\\"74\\\":\\\"a\\\",\\\"75\\\":\\\"r\\\",\\\"76\\\":\\\"t\\\",\\\"77\\\":\\\"i\\\",\\\"78\\\":\\\"n\\\",\\\"79\\\":\\\".\\\",\\\"80\\\":\\\">\\\"},\\\"token_ids\\\":{\\\"0\\\":1,\\\"1\\\":72,\\\"2\\\":101,\\\"3\\\":105,\\\"4\\\":114,\\\"5\\\":105,\\\"6\\\":118,\\\"7\\\":125,\\\"8\\\":119,\\\"9\\\":36,\\\"10\\\":88,\\\"11\\\":101,\\\"12\\\":118,\\\"13\\\":107,\\\"14\\\":101,\\\"15\\\":118,\\\"16\\\":125,\\\"17\\\":105,\\\"18\\\":114,\\\"19\\\":36,\\\"20\\\":109,\\\"21\\\":119,\\\"22\\\":36,\\\"23\\\":109,\\\"24\\\":114,\\\"25\\\":36,\\\"26\\\":75,\\\"27\\\":101,\\\"28\\\":113,\\\"29\\\":105,\\\"30\\\":36,\\\"31\\\":115,\\\"32\\\":106,\\\"33\\\":36,\\\"34\\\":88,\\\"35\\\":108,\\\"36\\\":118,\\\"37\\\":115,\\\"38\\\":114,\\\"39\\\":105,\\\"40\\\":119,\\\"41\\\":48,\\\"42\\\":36,\\\"43\\\":101,\\\"44\\\":36,\\\"45\\\":106,\\\"46\\\":101,\\\"47\\\":114,\\\"48\\\":120,\\\"49\\\":101,\\\"50\\\":119,\\\"51\\\":125,\\\"52\\\":36,\\\"53\\\":105,\\\"54\\\":116,\\\"55\\\":109,\\\"56\\\":103,\\\"57\\\":36,\\\"58\\\":102,\\\"59\\\":125,\\\"60\\\":36,\\\"61\\\":75,\\\"62\\\":105,\\\"63\\\":115,\\\"64\\\":118,\\\"65\\\":107,\\\"66\\\":105,\\\"67\\\":36,\\\"68\\\":86,\\\"69\\\":50,\\\"70\\\":86,\\\"71\\\":50,\\\"72\\\":36,\\\"73\\\":81,\\\"74\\\":101,\\\"75\\\":118,\\\"76\\\":120,\\\"77\\\":109,\\\"78\\\":114,\\\"79\\\":50,\\\"80\\\":2},\\\"entropies\\\":{\\\"0\\\":3.3949158192,\\\"1\\\":2.1656451225,\\\"2\\\":2.3216569424,\\\"3\\\":2.8214058876,\\\"4\\\":1.5249242783,\\\"5\\\":0.0401624143,\\\"6\\\":0.0981037766,\\\"7\\\":0.0544578359,\\\"8\\\":0.3430138826,\\\"9\\\":1.0546212196,\\\"10\\\":0.25252828,\\\"11\\\":0.1494535804,\\\"12\\\":0.0624754503,\\\"13\\\":0.001355894,\\\"14\\\":0.0050173439,\\\"15\\\":0.0052358187,\\\"16\\\":0.0011725067,\\\"17\\\":0.0010307421,\\\"18\\\":1.0241208076,\\\"19\\\":3.6867966652,\\\"20\\\":0.4502205253,\\\"21\\\":0.0484119244,\\\"22\\\":2.2572875023,\\\"23\\\":0.3789347112,\\\"24\\\":1.0042934418,\\\"25\\\":2.9090054035,\\\"26\\\":1.8933598995,\\\"27\\\":1.3859074116,\\\"28\\\":0.3827198744,\\\"29\\\":0.2646365762,\\\"30\\\":1.7742085457,\\\"31\\\":0.0136727821,\\\"32\\\":0.0053820172,\\\"33\\\":0.5485631227,\\\"34\\\":0.2064044327,\\\"35\\\":0.0049266233,\\\"36\\\":0.0005439016,\\\"37\\\":0.0007023578,\\\"38\\\":0.0004170335,\\\"39\\\":0.0054524317,\\\"40\\\":1.1938130856,\\\"41\\\":0.0238215197,\\\"42\\\":3.1279797554,\\\"43\\\":1.3883389235,\\\"44\\\":3.0503094196,\\\"45\\\":1.695879817,\\\"46\\\":1.8551058769,\\\"47\\\":1.4570231438,\\\"48\\\":0.0047810897,\\\"49\\\":0.026396824,\\\"50\\\":0.6633765101,\\\"51\\\":0.3141393065,\\\"52\\\":2.8411159515,\\\"53\\\":1.143143177,\\\"54\\\":0.0520330966,\\\"55\\\":0.3398066461,\\\"56\\\":0.4140175879,\\\"57\\\":2.5563707352,\\\"58\\\":1.3370712996,\\\"59\\\":0.0227173548,\\\"60\\\":3.4447185993,\\\"61\\\":1.8576486111,\\\"62\\\":0.8189754486,\\\"63\\\":0.6776530743,\\\"64\\\":0.0677763447,\\\"65\\\":0.212713033,\\\"66\\\":0.1003480032,\\\"67\\\":0.1746164262,\\\"68\\\":0.4123829603,\\\"69\\\":0.5507118702,\\\"70\\\":0.1047425047,\\\"71\\\":0.0194335245,\\\"72\\\":0.001482119,\\\"73\\\":0.0009310447,\\\"74\\\":0.0002176317,\\\"75\\\":0.0076908777,\\\"76\\\":0.0003866984,\\\"77\\\":0.0008008487,\\\"78\\\":1.2395234108,\\\"79\\\":0.4564163089,\\\"80\\\":0.0000461392},\\\"patch\\\":{\\\"0\\\":0,\\\"1\\\":1,\\\"2\\\":2,\\\"3\\\":3,\\\"4\\\":4,\\\"5\\\":5,\\\"6\\\":5,\\\"7\\\":5,\\\"8\\\":5,\\\"9\\\":5,\\\"10\\\":5,\\\"11\\\":5,\\\"12\\\":5,\\\"13\\\":5,\\\"14\\\":5,\\\"15\\\":5,\\\"16\\\":5,\\\"17\\\":5,\\\"18\\\":5,\\\"19\\\":5,\\\"20\\\":6,\\\"21\\\":6,\\\"22\\\":6,\\\"23\\\":7,\\\"24\\\":7,\\\"25\\\":7,\\\"26\\\":8,\\\"27\\\":9,\\\"28\\\":10,\\\"29\\\":10,\\\"30\\\":10,\\\"31\\\":11,\\\"32\\\":11,\\\"33\\\":11,\\\"34\\\":11,\\\"35\\\":11,\\\"36\\\":11,\\\"37\\\":11,\\\"38\\\":11,\\\"39\\\":11,\\\"40\\\":11,\\\"41\\\":11,\\\"42\\\":11,\\\"43\\\":12,\\\"44\\\":13,\\\"45\\\":14,\\\"46\\\":15,\\\"47\\\":16,\\\"48\\\":17,\\\"49\\\":17,\\\"50\\\":17,\\\"51\\\":17,\\\"52\\\":17,\\\"53\\\":18,\\\"54\\\":18,\\\"55\\\":18,\\\"56\\\":18,\\\"57\\\":18,\\\"58\\\":19,\\\"59\\\":20,\\\"60\\\":20,\\\"61\\\":21,\\\"62\\\":22,\\\"63\\\":22,\\\"64\\\":22,\\\"65\\\":22,\\\"66\\\":22,\\\"67\\\":22,\\\"68\\\":22,\\\"69\\\":22,\\\"70\\\":22,\\\"71\\\":22,\\\"72\\\":22,\\\"73\\\":22,\\\"74\\\":22,\\\"75\\\":22,\\\"76\\\":22,\\\"77\\\":22,\\\"78\\\":22,\\\"79\\\":22,\\\"80\\\":22},\\\"start\\\":{\\\"0\\\":1,\\\"1\\\":1,\\\"2\\\":1,\\\"3\\\":1,\\\"4\\\":1,\\\"5\\\":1,\\\"6\\\":0,\\\"7\\\":0,\\\"8\\\":0,\\\"9\\\":0,\\\"10\\\":0,\\\"11\\\":0,\\\"12\\\":0,\\\"13\\\":0,\\\"14\\\":0,\\\"15\\\":0,\\\"16\\\":0,\\\"17\\\":0,\\\"18\\\":0,\\\"19\\\":0,\\\"20\\\":1,\\\"21\\\":0,\\\"22\\\":0,\\\"23\\\":1,\\\"24\\\":0,\\\"25\\\":0,\\\"26\\\":1,\\\"27\\\":1,\\\"28\\\":1,\\\"29\\\":0,\\\"30\\\":0,\\\"31\\\":1,\\\"32\\\":0,\\\"33\\\":0,\\\"34\\\":0,\\\"35\\\":0,\\\"36\\\":0,\\\"37\\\":0,\\\"38\\\":0,\\\"39\\\":0,\\\"40\\\":0,\\\"41\\\":0,\\\"42\\\":0,\\\"43\\\":1,\\\"44\\\":1,\\\"45\\\":1,\\\"46\\\":1,\\\"47\\\":1,\\\"48\\\":1,\\\"49\\\":0,\\\"50\\\":0,\\\"51\\\":0,\\\"52\\\":0,\\\"53\\\":1,\\\"54\\\":0,\\\"55\\\":0,\\\"56\\\":0,\\\"57\\\":0,\\\"58\\\":1,\\\"59\\\":1,\\\"60\\\":0,\\\"61\\\":1,\\\"62\\\":1,\\\"63\\\":0,\\\"64\\\":0,\\\"65\\\":0,\\\"66\\\":0,\\\"67\\\":0,\\\"68\\\":0,\\\"69\\\":0,\\\"70\\\":0,\\\"71\\\":0,\\\"72\\\":0,\\\"73\\\":0,\\\"74\\\":0,\\\"75\\\":0,\\\"76\\\":0,\\\"77\\\":0,\\\"78\\\":0,\\\"79\\\":0,\\\"80\\\":0}}\"}'''\n",
    "\n",
    "# --- Parse the JSON ---\n",
    "data = json.loads(meta_json_str)\n",
    "df = pd.read_json(data[\"dataframe_json\"])\n",
    "print(\"Loaded DataFrame:\")\n",
    "\n",
    "# --- Set the threshold ---\n",
    "threshold = data[\"threshold\"]\n",
    "\n",
    "# --- Create x-axis tick labels combining position and token ---\n",
    "x_ticks = []\n",
    "for row in df.itertuples():\n",
    "    pos = row.position\n",
    "    token = row.tokens\n",
    "    x_ticks.append(f\"{str(pos).zfill(3)}|{token}\")\n",
    "df[\"position_with_token\"] = x_ticks\n",
    "\n",
    "# --- Define the Altair x-axis ---\n",
    "x_axis = alt.Axis(\n",
    "    labelExpr=\"split(datum.label, '|')[1]\",\n",
    "    grid=False,\n",
    "    labelOverlap=False,\n",
    "    labelAngle=0,\n",
    ")\n",
    "\n",
    "# --- Create the chart ---\n",
    "width = 1200\n",
    "height = 150\n",
    "base = alt.Chart(df).properties(width=width, height=height)\n",
    "\n",
    "points = base.mark_line(point=True).encode(\n",
    "    x=alt.X(\"position_with_token:O\", title=None, axis=x_axis),\n",
    "    y=alt.Y(\"entropies\", title=\"Entropy of Next Byte\"),\n",
    ")\n",
    "\n",
    "rule = base.mark_rule(color=\"red\", strokeDash=[4, 4]).encode(\n",
    "    y=alt.datum(threshold),\n",
    ")\n",
    "\n",
    "patch_rules = (\n",
    "    alt.Chart(df[df[\"start\"] > 0])\n",
    "    .mark_rule(color=\"#474747\", strokeDash=[4, 2])\n",
    "    .encode(x=alt.X(\"position_with_token:O\", axis=x_axis))\n",
    ")\n",
    "\n",
    "chart = patch_rules + rule + points\n",
    "chart = chart.configure_axis(labelFontSize=15, titleFontSize=15)\n",
    "\n",
    "output_path = Path(\"meta_chart.png\")\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "chart.save(str(output_path))\n",
    "print(f\"Chart saved to {output_path}\")\n",
    "\n",
    "print(f\"threshold: {threshold}\")\n",
    "print(f\"Mean entropy (metas model): {df['entropies'].mean()}\")\n",
    "print(f\"Total patches (metas model): {df['patch'].max()}\")\n",
    "chart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
